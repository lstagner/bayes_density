\NeedsTeXFormat{LaTeX2e}
\documentclass	[12pt]{article}
\usepackage[pdftex]{color,graphicx}
\usepackage{amsmath}
\usepackage{graphicx,caption,subcaption}
\numberwithin{equation}{section}
\begin{document}

\title{\bf{Integrated Data Analysis of the DIII-D Density Profile}}
\author{L. Stagner \& W.W. Heidbrink \\ University of California--Irvine, Irvine, California, USA}
\date{}
\maketitle
\begin{abstract}
\end{abstract}
\section{Introduction}
Introstuff

\section{Integrated Data Analysis}
As fusion experiments expand, so does the number of diagnostics used to 
probe the underlying physics. With the increase in diagnostics the 
number of interrelations among the diagnostics increases accordingly. 
While each diagnostic can act independently it is often advantageous to 
combine the data sets to acquire the most accurate result that is 
consistent with all the information available. This causes difficulties 
when trying to combine multiple data sets that cover complementary 
areas of interest. Bayesian data analysis give a comprehensive, 
scalable, and automated framework for combining complementary 
diagnostics.    
\subsection{The Bayesian perspective}
Bayesian statistics differs from the prevailing frequentist statistics 
that is taught in most introductory courses. The difference between the 
two views is in the definition of probability. In the Bayesian 
perspective, probability represented a degree of belief or plausibility 
that an event was true, based on the information available. This school 
of thought is opposed to frequentist view that probability is a long 
run relative frequency with which the event occurs, given infinitely 
many repeated experimental trials.\cite{von2011bayesian}

While the frequentist view is seemingly more objective, it is limited 
in its straightforward application since some things cannot have long 
run relative frequencies and, as a result, cannot be analysed by 
probability theory. For instance a constant, such as the mass of an 
object, cannot be analysed by a straightforward application of the 
frequentist definition of probability as a long run relative frequency. 
To use the frequentist view you first have to relate the mass to the 
data through some function called the \emph{statistic}. Since 
statistic is subject to random noise it becomes the random variable to 
which the rules of probability can then be applied. The issue with this 
approach is that there is no natural way of choosing the best 
statistic.\cite{sivia2006data} The founders of the field have a 	
plethora of ad hoc rules and tests to determine which statistic should 
be used in any situation. These rules and tests hides much of the 	
mechanics and assumptions inherent in the analysis and the consequences 
can often turn up at the worst possible moments.

The bayesian definition has no such difficulty since nearly anything 
can be thought of as a probability and the framework explicitly 
formulates all assumptions. That is not to say that it is without its 
issues. The frequentist view is often much easier to use because it 
hides most of the difficulties from the user. In bayesian statistics 
you have to be very careful with the assumptions that are made since it 
can drastically affect the results. Also, the calculation of optimal 
parameters is also a challenging numerical exercise in Bayesian 
statistics. It was partially for this reason that the frequentist view 
arose into prominence. It was only in the mid 20\textsuperscript{th} 
century with the advent of computers and efficient sampling algorithms 
that Bayesian statistics has begun its resurgence. In recent years 
Bayesian analysis has become essential to many scientific fields such 
as cosmology and artificial intelligence.\cite{von2011bayesian}
\subsection{Mathematical Formulation}
In 1946, Richard Cox began to formulate the rules for logical and consistent
reasoning by considering how we might quantify our beliefs about the truth of 
an event or object. He concluded that the real numbers we attached to our
beliefs followed the rules of probability:\cite{sivia2006data} 
\begin{equation} \label{eq:sumrule}
	prob(X|I) + prob(\overline{X}|I) = 1
\end{equation}
and 
\begin{equation} \label{eq:productrule}
	prob(X,Y|I) = prob(X|Y,I) \times prob(Y|X,I)
\end{equation}
Here $\overline{X}$ denotes that $X$ is false and the vertical bar `$|$' means `given'. 
All the probabilities are conditional on $I$ to denote the relevant background information 
available, since there is no such thing as an absolute probability. 

Equation \ref{eq:sumrule} and \ref{eq:productrule} are called the sum and product 
rules, respectively. The sum and product rule form the basic equations of probability 
theory. From these two rules we can derive the core equations used in bayesian 
statistics: Bayes' theorem
\begin{equation} \label{eq:bayes1}
	prob(X|Y,I) = \frac{prob(Y|X,I) \times prob(X|I)}{prob(Y|I)}
\end{equation}
and marginalization.
\begin{equation} \label{eq:marginalization}
	prob(X|I) = \int\limit_{-\infty}^{\infty} prob(X,Y|I)dY
\end{equation}

The power behind Bayes theorem can best be recognized with a change of notation. 
\begin{equation} \label{eq:bayes2}
	prob(hypothesis|\{data\},I) = \frac{prob(\{data\}|hypothesis,I) \times prob(hypothesis|I)}{prob(\{data\}|I)}
\end{equation}
The terms in the above equation have formal names. $prob(hypothesis|I)$ is 
called the \emph{prior} and is often denoted in parameter estimation problems as 
$\pi(\vec{\alpha}|I)$ where $\vec{\alpha}$ are the parameters to be inferred.
The prior encodes the current state of knowledge about the parameters before 
we have analyzed any new data. When we have acquired new information the prior 
is modified by the first term, $prob(\{data\}|hypothesis,I)$ called the 
\emph{likelihood} also denoted as $\mathcal{L}(\vec{d}|\vec{\alpha},I)$.
The product of the likelihood and the prior give $prob(hypothesis|\{data\},I)$, 
which is called the \emph{posterior} and is also denoted as 
$\mathcal{P}(\vec{\alpha}|\vec{d},I)$. The posterior encodes our final state of knowledge 
after all the data has been incorporated.
The maximum value of the posterior gives us the best estimate of the parameters we trying to infer.
The last term, $prob(\{data\}|I)$, is called the \emph{evidence} or \emph{marginal likelihood} 
and is denoted as $\mathcal{Z}$. In parameter estimation problems the evidence can be ignored since 
it is essentially a normalization constant. However, in model comparison problems it is of vital importance.
\subsection{Model Comparison}

\subsection{Choosing Priors}
It was noted earlier that one has to be extra careful with the assumptions that are made prior 
to incorporation of data. Quite fittingly, these assumptions are encoded in the choice of prior. 
This process can be difficult and if often done incorrectly. In order to choose the best prior 
three main schools of thought for choosing priors have been developed. 

The first school of thought is to choose a prior that expresses specific, definite information about a parameter.
If, for instance, a parameter is known, through past experience or expert testimony, to have a value 
around $1 \pm 0.2$ with upper bound of $2$. It would not be uncommon for someone to assign a prior that is a Gaussian with a mean of 
$1$ and standard deviation of $0.2$ that is zero for values greater than $2$. Informative priors are both a major advantage and can 
also be a major pitfall. If our prior knowledge was incorrect and the upper bound is around $5$, depending on the amount of data, we 
may have biased the posterior such that it doesn't reflect the truth. We must then be careful not to bias our results without proper 
justification. Informative priors also suffer from the fact that there is not a well defined procedure for choosing them. 

The second school of thought is to be as non-informative as possible. This method should 
be used when we have no prior information about the parameter in question. The most common type of non-informative prior 
is called the Jeffreys' Prior. The key property of the Jeffreys' Prior is that it is invariant under re-parameterization. 
The Jeffreys' prior is defined as 
\begin{equation} \label{eq:jefferys}
	\pi(\vec{\alpha}) \propto \sqrt{det \,\mathcal{I}(\vec{\alpha})}
\end{equation}
where $\mathcal{I}$ is the Fisher information. One of the main problems with the Jefferys' Prior is that it can be 
improper in the sense that it cannot normalizable to one (although there are ways around this). Improper priors should 
not be used since it can lead to paradoxes.\cite{von2011bayesian}
The Jefferys' Prior is also sometimes impossible to calculate and is therefore of limited use.

The third school of thought is to choose the prior that has the largest Shannon information entropy that is still consistent with the 
given testable information. The idea is that entropy is a measure of ``uninformativeness'' so picking the most `uninformative' prior 
that is still consistent with testable information would be the best choice. Mathematically, this the constrained optimization 
problem:
\begin{equation} \label{eq:maxent}
	Q = -\int \pi(x) log(\frac{\pi(x)}{m(x)})dx - \lambda C(x) 
\end{equation}
where $p(x)$ is the prior, $m(x)$ is the Lebesgue measure which ensures invariance under transformation, $\lambda$ is the Lagrange 
multiplier, and $C(x)$ is the constraint function. For example, if we knew the mean of a parameter, using the 
principle of maximum entropy the best prior would be a Poisson distribution. Likewise, if we knew both the mean and the variance of 
the parameter the principle would yield a gaussian. The principle of maximum entropy provides a balanced and systematic approach to 
problem of picking the right prior. It is for these reasons that we will endeavor to apply the principle of maximum entropy 
whenever possible.   
\subsection{Combining multiple diagnostics}
By applying the product rule multiple
\section{Forward modeling of density diagnostics}
stuff
\subsection{Functional form of the density profile}
stuff
\subsection{Interferometry}
stuff
\subsection{Reflectometry}
stuff
\subsection{Thomson scattering}
stuff
\subsection{Beam emission spectroscopy}
stuff
\section{Likelihood and prior probabilities}
stuff
\subsection{Priors}
stuff
\subsubsection{Parameter space priors}
stuff
\subsubsection{Data space priors}
stuff
\subsection{Likelihoods}
stuff
\subsubsection{Interferometry}
stuff
\subsubsection{Reflectometry}
stuff
\subsubsection{Thomson scattering}
stuff
\subsubsection{Beam emission spectroscopy}
stuff
\subsubsection{Combined likelihood probability}
stuff
\section{Total posterior probability}
stuff
\subsection{Exploring the posterior}
stuff
\section{Representation of Error}
stuff
\section{Density profile reconstructions}
stuff
\subsection{H-mode reconstruction}
stuff
\begin{figure}[ht]
	\centering
	\includegraphics[width=12cm,keepaspectratio=true]{figures/bfit146102_00505_all5}
	\vspace{-30pt}
	\caption{Reconstruction}
\label{hi}
\end{figure}
otherstuff
\subsection{L-mode reconstruction}
stuff
\section{Determining discrepant diagnostics}
stuff
\begin{figure}[h!]
	\centering
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth,keepaspectratio=true]{figures/bfit146102_00505_thom5}
		\vspace{-30pt}
		\caption{Thomson scattering}
		\label{fig:ts505}
	\end{subfigure}
	\hspace{-20pt}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth,keepaspectratio=true]{figures/bfit146102_00505_inter5}
		\vspace{-30pt}
		\caption{Interferometer}
		\label{fig:inter505}
	\end{subfigure} \\%  ------- End of the first row ----------------------%
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth,keepaspectratio=true]{figures/bfit146102_00505_refl5}
		\vspace{-30pt}
		\caption{Reflectometry}
		\label{fig:refl505}
	\end{subfigure}
	\hspace{-20pt}
	\begin{subfigure}[b]{0.5\textwidth}
		\centering
		\includegraphics[width=\textwidth,keepaspectratio=true]{figures/bfit146102_00505_all5}
		\vspace{-30pt}
		\caption{Combined}
		\label{fig:all505}
	\end{subfigure}
	\caption{The l-o-n-g caption for all the subfigures (FirstFigure through FourthFigure) goes here.}
	\label{fig:dne505}
\end{figure}
\section{Conclusions and future work}
stuff

\bibliographystyle{unsrt}
\bibliography{bayes,algorithms,diag}
\end{document}
